# ========================================
# Configuration - Startup Incubation API
# ========================================

# ─────────────────────────────────────
# Serveur
# ─────────────────────────────────────
HOST=0.0.0.0
PORT=5005

# ─────────────────────────────────────
# Ollama Configuration
# ─────────────────────────────────────
# URL de base du serveur Ollama
# Dev: http://localhost:11434
# Prod: http://ollama-service:11434 (si dans Docker)
OLLAMA_URL=http://localhost:11434

# Modèle LLM à utiliser pour la génération
# Recommandations:
# - llama3.1:3b-instruct (rapide, léger, bon pour production)
# - mistral:7b-instruct-q4_K_M (meilleure qualité, plus lent)
# - llama3.1:8b-instruct (équilibre qualité/vitesse)
LLM_MODEL=llama3.1:3b-instruct

# Modèle d'embeddings pour le RAG
# - nomic-embed-text (recommandé, rapide)
# - all-minilm (alternative légère)
EMBED_MODEL=nomic-embed-text

# ─────────────────────────────────────
# Timeouts (en secondes)
# ─────────────────────────────────────
# Timeout maximum pour les requêtes LLM
MAX_LLM_TIMEOUT=12.0

# Timeout maximum pour les embeddings
MAX_EMBED_TIMEOUT=6.0

# ─────────────────────────────────────
# Cache et RAG
# ─────────────────────────────────────
# Nombre de réponses à garder en cache par startup/étape
CACHE_SIZE=6

# Nombre de snippets à récupérer pour le RAG
RAG_TOP_K=8

# Taille minimale d'un snippet (en caractères)
MIN_SNIPPET_CHARS=30

# Seuil de déduplication des snippets (0.0-1.0)
# Plus élevé = plus strict
DEDUPE_THRESHOLD=0.8

# Seuil de similarité pour la détection de duplication (0.0-1.0)
# Plus élevé = plus strict
SIMILARITY_THRESHOLD=0.90

# ─────────────────────────────────────
# CORS
# ─────────────────────────────────────
# Origines autorisées (séparées par des virgules)
# Dev: *
# Prod: https://votre-domaine.com,https://app.votre-domaine.com
CORS_ORIGINS=*

# ─────────────────────────────────────
# Logging
# ─────────────────────────────────────
# Niveau de log: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO