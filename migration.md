# üì¶ Guide de migration v2.7 ‚Üí v3.0

Ce guide explique comment migrer de l'ancienne architecture (v2.7) vers la nouvelle (v3.0).

## üéØ Changements majeurs

### 1. Architecture simplifi√©e

**Avant (v2.7) :**
```
app/
‚îú‚îÄ‚îÄ main.py           # Pipeline principal
‚îî‚îÄ‚îÄ routes/
    ‚îî‚îÄ‚îÄ analyse.py    # Endpoint /api/score s√©par√©
```

**Apr√®s (v3.0) :**
```
app/
‚îî‚îÄ‚îÄ main.py           # Application unifi√©e
```

‚úÖ **B√©n√©fice :** Code plus simple, maintenance facilit√©e

### 2. Configuration centralis√©e

**Avant :** Configuration dispers√©e dans le code

**Apr√®s :** Configuration via `.env` et classe `Config`

```python
# Avant
OLLAMA_URL = "http://localhost:11434"
LLM_MODEL = "mistral:7b-instruct-q4_K_M"

# Apr√®s
class Config:
    OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
    LLM_MODEL = os.getenv("LLM_MODEL", "llama3.1:3b-instruct")
```

### 3. Mod√®le LLM par d√©faut

**Avant :** `mistral:7b-instruct-q4_K_M` (lent)

**Apr√®s :** `llama3.1:3b-instruct` (rapide)

‚úÖ **B√©n√©fice :** R√©ponses 2-3x plus rapides

### 4. Nouveau syst√®me de scoring

**Avant :** `/api/score` avec analyse LLM lente

**Apr√®s :** `/api/score_v2` avec r√®gles m√©tier rapides

## üîÑ √âtapes de migration

### √âtape 1 : Backup

```bash
# Sauvegarder l'ancienne version
cp -r app app.backup
cp requirements.txt requirements.txt.backup
cp dockerfile dockerfile.backup
```

### √âtape 2 : Remplacer les fichiers

```bash
# Supprimer l'ancienne structure
rm -rf app/
rm requirements.txt
rm dockerfile

# Copier les nouveaux fichiers
# (voir les artifacts fournis)
```

### √âtape 3 : Configuration

```bash
# Cr√©er le fichier .env
cp .env.example .env

# √âditer .env avec vos param√®tres
nano .env
```

**Configuration minimale :**
```env
OLLAMA_URL=http://localhost:11434
LLM_MODEL=llama3.1:3b-instruct
EMBED_MODEL=nomic-embed-text
PORT=5005
```

### √âtape 4 : Mod√®les Ollama

```bash
# T√©l√©charger les nouveaux mod√®les (si n√©cessaire)
ollama pull llama3.1:3b-instruct
ollama pull nomic-embed-text

# Optionnel : supprimer l'ancien mod√®le pour lib√©rer de l'espace
ollama rm mistral:7b-instruct-q4_K_M
```

### √âtape 5 : Installation

```bash
# Cr√©er un nouvel environnement virtuel
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou .venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### √âtape 6 : Tests

```bash
# Lancer l'application
python app/main.py

# Tester le health check
curl http://localhost:5005/health

# Tester la g√©n√©ration
curl -X POST http://localhost:5005/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "step_name": "Test",
    "question_label": "Test question",
    "context": "Startup: TestCo"
  }'
```

## üîå Adaptation de l'int√©gration Laravel

### Changements dans les appels API

#### 1. Endpoint de g√©n√©ration (inchang√©)

```php
// ‚úÖ Pas de changement
$response = Http::timeout(30)->post('http://localhost:5005/api/generate', [
    'step_name' => $step,
    'question_label' => $question,
    'context' => $context,
    'question_type' => $type,
    'prompt' => $prompt
]);
```

#### 2. Endpoint de scoring (nouveau)

**Avant :**
```php
// ‚ùå Ancien endpoint lent
$response = Http::timeout(150)->post('http://localhost:5005/api/score', [
    'step_name' => $step,
    'responses' => $responses,
    'questions' => $questions
]);
```

**Apr√®s (recommand√©) :**
```php
// ‚úÖ Nouveau endpoint rapide et structur√©
$items = [];
foreach ($questions as $id => $question) {
    $items[] = [
        'question_id' => $id,
        'label' => $question->label,
        'type' => $question->type,
        'answer' => $answers[$id],
        'points' => $question->points ?? 10,
        'has_file' => !empty($files[$id])
    ];
}

$response = Http::timeout(30)->post('http://localhost:5005/api/score_v2', [
    'step_name' => $step,
    'items' => $items
]);

$result = $response->json();
// $result['global_score'] : score global (0-100)
// $result['status'] : 'valid√©e' ou '√† retravailler'
// $result['items'][i]['score'] : score par question
```

**Fallback (compatibilit√©) :**
```php
// ‚ö†Ô∏è L'ancien endpoint existe toujours pour compatibilit√©
$response = Http::timeout(30)->post('http://localhost:5005/api/score', [
    'step_name' => $step,
    'responses' => $responses,
    'questions' => $questions
]);
// Mais utilise un algorithme simplifi√©
```

### Exemple d'int√©gration compl√®te

```php
<?php

namespace App\Services;

use Illuminate\Support\Facades\Http;

class AIAnalysisService
{
    private string $baseUrl;

    public function __construct()
    {
        $this->baseUrl = config('services.ai_analysis.url', 'http://localhost:5005');
    }

    /**
     * G√©n√®re une r√©ponse pour une question
     */
    public function generate(
        string $stepName,
        string $questionLabel,
        string $context,
        string $questionType = 'text',
        string $prompt = ''
    ): ?string {
        try {
            $response = Http::timeout(30)
                ->withHeaders(['X-Deadline' => (time() + 25) * 1000]) // Deadline
                ->post("{$this->baseUrl}/api/generate", [
                    'step_name' => $stepName,
                    'question_label' => $questionLabel,
                    'context' => $context,
                    'question_type' => $questionType,
                    'prompt' => $prompt
                ]);

            if ($response->successful()) {
                return $response->json()['answer'] ?? null;
            }

            \Log::error('AI generation failed', [
                'status' => $response->status(),
                'body' => $response->body()
            ]);

            return null;
        } catch (\Exception $e) {
            \Log::error('AI generation exception', ['error' => $e->getMessage()]);
            return null;
        }
    }

    /**
     * Score un ensemble de r√©ponses (v2)
     */
    public function scoreV2(string $stepName, array $items): ?array
    {
        try {
            $response = Http::timeout(30)->post("{$this->baseUrl}/api/score_v2", [
                'step_name' => $stepName,
                'items' => $items
            ]);

            if ($response->successful()) {
                return $response->json();
            }

            return null;
        } catch (\Exception $e) {
            \Log::error('AI scoring exception', ['error' => $e->getMessage()]);
            return null;
        }
    }

    /**
     * V√©rifie l'√©tat de l'API
     */
    public function health(): bool
    {
        try {
            $response = Http::timeout(5)->get("{$this->baseUrl}/health");
            return $response->successful() && 
                   $response->json()['status'] === 'ok';
        } catch (\Exception $e) {
            return false;
        }
    }
}
```

## üìä Comparaison des performances

| Op√©ration | v2.7 | v3.0 | Am√©lioration |
|-----------|------|------|--------------|
| Health check | 0.1s | 0.1s | = |
| Generate (sans RAG) | 5-8s | 2-4s | **2x plus rapide** |
| Generate (avec RAG) | 8-12s | 3-8s | **~2x plus rapide** |
| Score (LLM) | 60-150s | - | Deprecated |
| Score (v2, r√®gles) | - | <0.5s | **Nouveau, ultra-rapide** |

## üêõ Probl√®mes courants

### Probl√®me 1 : Ollama ne trouve pas le mod√®le

**Erreur :**
```
Error: model 'llama3.1:3b-instruct' not found
```

**Solution :**
```bash
ollama pull llama3.1:3b-instruct
```

### Probl√®me 2 : Port 5005 d√©j√† utilis√©

**Erreur :**
```
OSError: [Errno 48] Address already in use
```

**Solution :**
```bash
# Trouver le processus
lsof -i :5005

# Tuer le processus
kill -9 <PID>

# Ou changer le port dans .env
PORT=5006
```

### Probl√®me 3 : R√©ponses vides

**Causes possibles :**
1. Ollama non d√©marr√©
2. Mod√®le non t√©l√©charg√©
3. Timeout trop court

**Solution :**
```bash
# V√©rifier Ollama
ollama list

# V√©rifier la connexion
curl http://localhost:11434/api/tags

# Augmenter le timeout dans .env
MAX_LLM_TIMEOUT=15.0
```

### Probl√®me 4 : CORS errors

**Solution :**
```env
# Dans .env, ajouter votre origine
CORS_ORIGINS=http://localhost:8000,https://votre-domaine.com
```

## üìù Checklist de migration

- [ ] Backup de l'ancienne version
- [ ] Nouveaux fichiers copi√©s
- [ ] `.env` configur√©
- [ ] Mod√®les Ollama t√©l√©charg√©s
- [ ] D√©pendances install√©es
- [ ] Tests de l'API r√©ussis
- [ ] Int√©gration Laravel adapt√©e (si n√©cessaire)
- [ ] Tests E2E r√©ussis
- [ ] Documentation mise √† jour
- [ ] Monitoring configur√© (optionnel)

## üéâ Apr√®s la migration

### Optimisations recommand√©es

1. **GPU pour Ollama** (si disponible)
   ```bash
   # V√©rifier GPU
   nvidia-smi
   
   # Configurer Ollama pour utiliser GPU
   # Automatique sur la plupart des syst√®mes
   ```

2. **Monitoring** (optionnel)
   ```bash
   pip install prometheus-fastapi-instrumentator
   ```

3. **Cache Redis** (pour haute charge)
   ```python
   # TODO: Impl√©menter cache Redis pour les embeddings
   ```

### Support

Pour toute question :
- Documentation : [README.md](README.md)
- Algorithme : [ALGORITHM.md](ALGORITHM.md)
- Issues : Cr√©er une issue sur le repo

## üìÑ Licence

Migration guide pour Startup Incubation API v3.0.0
