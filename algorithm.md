# üß† Algorithme d√©taill√© - Startup Incubation API

Ce document explique en d√©tail le fonctionnement des algorithmes utilis√©s dans l'API.

## Table des mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Pipeline de g√©n√©ration](#pipeline-de-g√©n√©ration)
- [Pipeline de scoring](#pipeline-de-scoring)
- [Composants cl√©s](#composants-cl√©s)
- [Optimisations](#optimisations)

## Vue d'ensemble

L'API utilise une combinaison de techniques d'IA :

1. **RAG** (Retrieval-Augmented Generation) : Pour enrichir le contexte
2. **LLM** (Large Language Model) : Pour g√©n√©rer du texte
3. **R√®gles m√©tier** : Pour valider et scorer
4. **Cache intelligent** : Pour √©viter les duplications

## Pipeline de g√©n√©ration

Le pipeline `/api/generate` suit 5 √©tapes principales :

### √âtape 1 : Parse du contexte

**Objectif :** Extraire les informations structur√©es du contexte Laravel.

**Input :**
```
Startup: MonStartup
Sector: SaaS B2B
Problem: Les PME perdent du temps avec la gestion manuelle
PrevAnswer: Qui est impact√© ? => PME B2B de 50-200 employ√©s
```

**Process :**

```python
def parse_context_lines(context: str):
    """
    1. Extrait les faits (Startup, Sector, Problem, etc.)
    2. Construit le corpus pour le RAG (toutes les lignes + PrevAnswer)
    """
    
    # Parsing des faits
    facts = {}
    for line in context.splitlines():
        if ":" in line:
            key, value = line.split(":", 1)
            facts[normalize(key)] = value.strip()
    
    # Construction du corpus pour RAG
    corpus = []
    for line in context.splitlines():
        if "PrevAnswer:" in line:
            # Format: PrevAnswer: Question => R√©ponse
            question, answer = parse_prev_answer(line)
            corpus.append((question, answer))
        elif ":" in line:
            key, value = line.split(":", 1)
            corpus.append((key, value))
    
    return facts, corpus
```

**Output :**
- `facts` : `{"startup": "MonStartup", "sector": "SaaS B2B", ...}`
- `corpus` : `[("Problem", "Les PME perdent..."), ("Qui est impact√© ?", "PME B2B...")]`

### √âtape 2 : RAG (Retrieval-Augmented Generation)

**Objectif :** Trouver les informations les plus pertinentes dans le corpus pour r√©pondre √† la question.

**Process :**

#### 2.1 Expansion de la requ√™te

```python
def expand_query(query: str) -> str:
    """
    Enrichit la requ√™te avec des synonymes
    """
    # Mapping de concepts
    expansions = {
        "proposition de valeur": ["UVP", "value prop", "promesse"],
        "canaux": ["acquisition", "distribution", "marketing"],
        "revenus": ["business model", "pricing", "mon√©tisation"]
    }
    
    expanded = query
    for concept, synonyms in expansions.items():
        if concept in query.lower():
            expanded += " " + " ".join(synonyms)
    
    return expanded
```

#### 2.2 G√©n√©ration des embeddings

```python
def build_rag_context(query: str, corpus: List[Tuple[str, str]]):
    """
    1. Filtrer les documents trop courts (< 30 chars)
    2. G√©n√©rer l'embedding de la requ√™te
    3. G√©n√©rer les embeddings des documents
    4. Calculer la similarit√© cosinus
    5. Trier et retourner les top-k
    """
    
    # 1. Filtrer
    docs = [(label, text) for label, text in corpus if len(text) >= 30]
    
    # 2. Embedding de la requ√™te
    query_expanded = expand_query(query)
    query_vec = embed_ollama([query_expanded])[0]
    
    # 3. Embeddings des documents
    doc_texts = [text for _, text in docs]
    doc_vecs = embed_ollama(doc_texts)
    
    # 4. Calcul des scores
    scores = []
    for (label, text), vec in zip(docs, doc_vecs):
        similarity = cosine_similarity(query_vec, vec)
        scores.append((label, text, similarity))
    
    # 5. Tri et s√©lection
    scores.sort(key=lambda x: x[2], reverse=True)
    top_k = scores[:8]  # Top 8
    
    # 6. D√©duplication
    deduplicated = remove_duplicates(top_k)
    
    return deduplicated
```

**Similarit√© cosinus :**

```
cos(A, B) = (A ¬∑ B) / (||A|| √ó ||B||)

O√π :
- A ¬∑ B = somme(Ai √ó Bi)  [produit scalaire]
- ||A|| = sqrt(somme(Ai¬≤))  [norme]
```

**Exemple :**

Requ√™te : "Qui est impact√© par le probl√®me ?"

Corpus :
1. "Problem: Les PME perdent du temps" ‚Üí score: 0.72
2. "Qui est impact√© ? => PME B2B de 50-200 employ√©s" ‚Üí score: 0.91 ‚úì
3. "Solution: Automatiser la gestion" ‚Üí score: 0.45
4. "Personas: Managers PME" ‚Üí score: 0.88 ‚úì

R√©sultat : Snippets 2 et 4 sont les plus pertinents.

#### 2.3 D√©duplication

```python
def dedupe_snippets(snippets: List[Tuple[str, str]]):
    """
    Supprime les snippets trop similaires entre eux
    """
    result = []
    for label, text in snippets:
        # V√©rifier si similaire √† un snippet d√©j√† ajout√©
        is_duplicate = False
        for _, existing_text in result:
            if jaccard_similarity(text, existing_text) > 0.8:
                is_duplicate = True
                break
        
        if not is_duplicate:
            result.append((label, text))
    
    return result
```

**Similarit√© de Jaccard (shingles) :**

```
J(A, B) = |A ‚à© B| / |A ‚à™ B|

O√π A et B sont des ensembles de n-grams (ex: 6 mots cons√©cutifs)
```

### √âtape 3 : G√©n√©ration LLM

**Objectif :** Utiliser le LLM pour g√©n√©rer une r√©ponse personnalis√©e.

#### 3.1 Construction du prompt

```python
def build_llm_prompt(step, question, qtype, style, facts, rag_snippets):
    """
    Construit un prompt complet et structur√©
    """
    
    prompt = f"""
√âtape: {step}
Question: {question}
Type: {qtype}
Style: {style}

=== Contexte pertinent ===
{format_snippets(rag_snippets)}

=== Faits bruts ===
- Startup: {facts['startup']}
- Sector: {facts['sector']}
- Problem: {facts['problem']}
...

=== Fragments √† NE PAS copier ===
{list_forbidden_fragments(rag_snippets)}

=== CONTRAT DE SORTIE ===
1) Adapter √† la startup et au secteur
2) Respecter le type de r√©ponse
3) Pas de placeholders [xxx]
4) Si hypoth√®ses ‚Üí terminer par "Assumptions & Next step: ..."

Consigne: r√©dige la r√©ponse maintenant.
"""
    
    return prompt
```

#### 3.2 Variation de style

Pour √©viter la r√©p√©tition, le style varie selon une seed d√©terministe :

```python
def _variation_seed(text: str) -> int:
    """G√©n√®re une seed stable √† partir du texte"""
    return int(hashlib.md5(text.encode()).hexdigest(), 16) % 10000

# Variantes de style
STYLE_VARIANTS = {
    "paragraph": [
        "paragraphe compact, ton pragmatique",
        "paragraphe orient√© r√©sultats",
        "paragraphe synth√©tique, ton business"
    ],
    "bullets": [
        "puces br√®ves, verbe d'action",
        "puces avec chiffres",
        "puces MECE, 4-6 items"
    ]
}

# S√©lection
seed = _variation_seed(startup_name + question)
rnd = random.Random(seed)
style_note = rnd.choice(STYLE_VARIANTS[style])
```

#### 3.3 Temp√©rature dynamique

```python
def _stable_jitter(seed_text: str):
    """
    G√©n√®re une temp√©rature entre 0.3 et 0.6
    bas√©e sur le hash du texte
    """
    hash_val = int(hashlib.md5(seed_text.encode()).hexdigest(), 16)
    normalized = (hash_val % 200 - 100) / 100.0  # -1.0 √† 1.0
    
    base = 0.45
    spread = 0.15
    temp = base + spread * normalized
    
    return max(0.1, min(0.95, temp))
```

Cette approche garantit :
- **Coh√©rence** : M√™me input ‚Üí m√™me temp√©rature
- **Variation** : Diff√©rents inputs ‚Üí diff√©rentes temp√©ratures
- **Contr√¥le** : Toujours dans une plage raisonnable

#### 3.4 Appel √† Ollama

```python
def llm_generate(prompt: str, system: str, max_seconds: float):
    """
    1. Essayer /api/chat (format conversationnel)
    2. Si √©chec, fallback sur /api/generate
    3. Timeout dynamique
    """
    
    # Calculer la temp√©rature
    temp = _stable_jitter(prompt[:128] + system[:128])
    
    # Options Ollama
    options = {
        "temperature": temp,
        "top_p": 0.9,
        "top_k": 40,
        "num_ctx": 4096,
        "repeat_penalty": 1.25,
        "repeat_last_n": 128
    }
    
    # Tentative 1 : /api/chat
    try:
        payload = {
            "model": "llama3.1:3b-instruct",
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": prompt}
            ],
            "options": options,
            "stream": False
        }
        response = requests.post(
            "http://localhost:11434/api/chat",
            json=payload,
            timeout=max_seconds
        )
        if response.ok:
            return response.json()["message"]["content"]
    except:
        pass
    
    # Tentative 2 : /api/generate
    payload = {
        "model": "llama3.1:3b-instruct",
        "prompt": prompt,
        "system": system,
        "options": options,
        "stream": False
    }
    response = requests.post(
        "http://localhost:11434/api/generate",
        json=payload,
        timeout=max_seconds
    )
    return response.json()["response"]
```

### √âtape 4 : Fallbacks bas√©s sur r√®gles

**Objectif :** Si le LLM ne r√©pond pas ou donne une r√©ponse trop courte, utiliser des r√®gles.

```python
def answer_for_question(question: str, facts: Dict):
    """
    R√®gles m√©tier pour r√©ponses rapides
    """
    q = question.lower()
    
    # R√®gle : Personas / Cible
    if "qui est impact√©" in q or "client cible" in q:
        if facts.get("personas"):
            return facts["personas"]
        if facts.get("customer_segments"):
            return facts["customer_segments"]
        return "D√©crivez 1-2 personas (profil, taille, zone)"
    
    # R√®gle : Proposition de valeur
    if "proposition de valeur" in q or "uvp" in q:
        return build_uvp_from_facts(facts)
    
    # R√®gle : Business model
    if "business model" in q:
        return facts.get("business_model") or \
               "Expliquez comment l'entreprise gagne de l'argent"
    
    # ... autres r√®gles
    
    return ""
```

**Exemple de construction UVP :**

```python
def build_uvp_from_facts(facts: Dict) -> str:
    """
    Construit une UVP √† partir des faits disponibles
    """
    if facts.get("value_prop"):
        return facts["value_prop"]
    
    parts = []
    if facts.get("problem"):
        parts.append(f"Probl√®me: {facts['problem']}")
    if facts.get("solution"):
        parts.append(f"Solution: {facts['solution']}")
    if facts.get("personas"):
        parts.append(f"Cible: {facts['personas']}")
    if facts.get("advantage"):
        parts.append(f"Diff√©renciation: {facts['advantage']}")
    
    return " ‚Äî ".join(parts)
```

### √âtape 5 : Post-traitement

#### 5.1 Nettoyage

```python
def smart_clean(text: str) -> str:
    """
    1. Supprimer les phrases avec placeholders [xxx]
    2. Normaliser les espaces
    3. Retourner le r√©sultat ou l'original
    """
    
    placeholders = [r"\[[^\]]+\]", r"\bX%\b", r"\bSegment X\b"]
    
    # Supprimer les phrases probl√©matiques
    for pattern in placeholders:
        if re.search(pattern, text):
            sentences = re.split(r"(?<=[\.\!\?])\s+", text)
            sentences = [s for s in sentences 
                        if not re.search(pattern, s)]
            text = " ".join(sentences)
    
    # Normaliser les espaces
    text = re.sub(r"\s{2,}", " ", text).strip()
    
    return text if text else original_text
```

#### 5.2 Variation d'ouverture

```python
def vary_opening(text: str, facts: Dict, question: str):
    """
    Ajoute un opener vari√© au d√©but de la r√©ponse
    """
    
    openers = [
        "Concr√®tement, ",
        "En pratique, ",
        "Pour cette startup, ",
        "Dans ce contexte, ",
        ""
    ]
    
    # S√©lection d√©terministe
    seed = facts["startup"] + question
    rnd = random.Random(_variation_seed(seed))
    opener = rnd.choice(openers)
    
    # Application
    if opener and not text.startswith(("‚àí", "-")):
        return opener + text[0].lower() + text[1:]
    
    return text
```

#### 5.3 Coercition de type

```python
def coerce_by_type(answer: str, qtype: str) -> str:
    """
    Adapte la r√©ponse au type attendu
    """
    
    if qtype == "number":
        # Extraire le premier nombre
        match = re.findall(r"-?\d+(?:[.,]\d+)?", answer)
        return match[0].replace(",", ".") if match else ""
    
    if qtype == "date":
        # Convertir en format ISO
        match = re.search(r"(\d{2})/(\d{2})/(\d{4})", answer)
        if match:
            return f"{match.group(3)}-{match.group(2)}-{match.group(1)}"
        return ""
    
    if qtype == "email":
        # Extraire l'email
        match = re.search(r"[\w\.-]+@[\w\.-]+\.\w+", answer)
        return match.group(0) if match else ""
    
    if qtype == "file":
        # Format: filename.ext ‚Äî description
        if "‚Äî" not in answer:
            slug = re.sub(r"[^a-z0-9]+", "-", answer[:40].lower())
            return f"{slug}.pdf ‚Äî fichier √† compl√©ter"
        return answer
    
    return answer
```

#### 5.4 Anti-duplication

**Probl√®me :** Le LLM peut g√©n√©rer la m√™me r√©ponse pour diff√©rentes questions.

**Solution :** Cache + d√©tection de similarit√©

```python
# Cache global
RECENT_CACHE = {}  # {startup-step: [r√©ponses r√©centes]}

def check_and_regenerate_if_duplicate(answer, facts, step, question):
    """
    1. Calculer la cl√© de cache
    2. Comparer avec les r√©ponses r√©centes
    3. Si trop similaire ‚Üí r√©g√©n√©rer
    """
    
    cache_key = f"{facts['startup']}-{step}"
    previous_answers = RECENT_CACHE.get(cache_key, [])
    
    # V√©rifier la similarit√©
    for prev in previous_answers:
        if jaccard_similarity(answer, prev) > 0.90:
            # Trop similaire ! R√©g√©n√©rer
            new_prompt = original_prompt + """
            
Consigne additionnelle: 
Change d'angle et de lexique. √âvite les m√™mes tournures.
Introduis un chiffre diff√©rent si plausible.
"""
            answer = llm_generate(new_prompt, ...)
            break
    
    # Stocker dans le cache
    RECENT_CACHE[cache_key].append(answer)
    if len(RECENT_CACHE[cache_key]) > 6:  # Garder seulement les 6 derni√®res
        RECENT_CACHE[cache_key] = RECENT_CACHE[cache_key][-6:]
    
    return answer
```

**Similarit√© de Jaccard avec shingles :**

```python
def jaccard_similarity(text1: str, text2: str) -> float:
    """
    Calcule la similarit√© entre deux textes
    en utilisant des shingles (n-grams de mots)
    """
    
    # Cr√©er des shingles (6-grams de mots)
    def shingles(text, n=6):
        words = re.findall(r"\w+", text.lower())
        return set(tuple(words[i:i+n]) 
                  for i in range(len(words) - n + 1))
    
    A = shingles(text1)
    B = shingles(text2)
    
    if not A or not B:
        return 0.0
    
    intersection = len(A & B)
    union = len(A | B)
    
    return intersection / union
```

**Exemple :**

```
Text1: "Les PME du secteur retail rencontrent des difficult√©s"
Text2: "Les PME du secteur retail rencontrent des probl√®mes"

Shingles (3-grams) de Text1:
- ("les", "pme", "du")
- ("pme", "du", "secteur")
- ("du", "secteur", "retail")
- ("secteur", "retail", "rencontrent")
- ("retail", "rencontrent", "des")
- ("rencontrent", "des", "difficult√©s")

Shingles de Text2:
- ("les", "pme", "du")
- ("pme", "du", "secteur")
- ("du", "secteur", "retail")
- ("secteur", "retail", "rencontrent")
- ("retail", "rencontrent", "des")
- ("rencontrent", "des", "probl√®mes")

Intersection: 5 shingles communs
Union: 7 shingles au total
Similarit√©: 5/7 = 0.71 (71%)
```

## Pipeline de scoring

Le pipeline `/api/score_v2` √©value la qualit√© des r√©ponses.

### √âtape 1 : Analyse du contenu

```python
def _score_item(step: str, item: Item) -> Dict:
    """
    Score un item selon son type et son contenu
    """
    
    text = item.answer.strip()
    base_score = 60  # Score de d√©part
    missing = []  # √âl√©ments manquants
    
    # 1. Analyse de base
    word_count = len(re.findall(r"\w+", text))
    has_numbers = bool(re.search(r"\d", text))
    has_percentage = "%" in text
    has_currency = "‚Ç¨" in text
    
    # 2. Ajustements selon le contenu
    if word_count < 18:
        base_score = 35  # Trop court
    if word_count >= 30:
        base_score += 10
    if word_count >= 50:
        base_score += 5
    if has_numbers:
        base_score += 5
    if has_percentage:
        base_score += 3
    if has_currency:
        base_score += 3
    
    # 3. V√©rifications sp√©cifiques
    question_lower = item.label.lower()
    
    if "qui est impact√©" in question_lower:
        # Doit mentionner un segment
        if not re.search(r"(client|pme|entreprise|segment)", text.lower()):
            missing.append("pr√©ciser le segment")
            base_score -= 10
        
        # Doit avoir des chiffres
        if not has_numbers:
            missing.append("indiquer un ordre de grandeur")
            base_score -= 10
    
    # 4. Score final
    score = max(0, min(100, base_score))
    
    return {
        "score": score,
        "missing": missing,
        ...
    }
```

### √âtape 2 : G√©n√©ration du feedback

```python
def generate_feedback(score: int, missing: List[str]) -> str:
    """
    G√©n√®re un feedback personnalis√©
    """
    
    if score >= 85:
        return "Excellent : r√©ponse pr√©cise, claire et exploitable."
    
    if score >= 70:
        feedback = "Bon niveau."
        if missing:
            feedback += " Rendez plus concrets : " + ", ".join(missing[:2])
        return feedback
    
    if score >= 50:
        return "Base correcte mais incompl√®te. Ajoutez des chiffres et un ancrage temporel."
    
    return "Insuffisant : structurez, quantifiez et citez au moins une r√©f√©rence."
```

### √âtape 3 : Score global pond√©r√©

```python
def calculate_global_score(items: List[Dict]) -> int:
    """
    Calcule le score global en pond√©rant par les points
    """
    
    total_points = sum(max(1, item["points"]) for item in items)
    
    if total_points == 0:
        return 0
    
    weighted_sum = sum(
        item["score"] * max(1, item["points"])
        for item in items
    )
    
    return round(weighted_sum / total_points)
```

**Exemple :**

```
Item 1: score=80, points=10 ‚Üí contribution = 800
Item 2: score=60, points=5  ‚Üí contribution = 300
Item 3: score=90, points=15 ‚Üí contribution = 1350

Total points = 30
Weighted sum = 2450
Global score = 2450 / 30 = 81.67 ‚âà 82
```

## Composants cl√©s

### 1. Gestion du temps (deadline-aware)

```python
def _seconds_left(deadline_ms: Optional[int]) -> float:
    """
    Calcule le temps restant avant la deadline
    
    Args:
        deadline_ms: Deadline en millisecondes (epoch)
    
    Returns:
        Secondes restantes (999.0 si pas de deadline)
    """
    if not deadline_ms:
        return 999.0
    
    now_ms = int(time.time() * 1000)
    return max(0.0, (deadline_ms - now_ms) / 1000.0)

# Utilisation dans le pipeline
if time_left() > 8.0:
    # Assez de temps pour le RAG
    rag_snippets = build_rag_context(...)

if time_left() > 4.5:
    # Assez de temps pour le LLM
    answer = llm_generate(..., max_seconds=time_left() - 1.0)
```

### 2. Configuration centralis√©e

```python
class Config:
    """Configuration via variables d'environnement"""
    OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
    LLM_MODEL = os.getenv("LLM_MODEL", "llama3.1:3b-instruct")
    EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text")
    MAX_LLM_TIMEOUT = float(os.getenv("MAX_LLM_TIMEOUT", "12.0"))
    RAG_TOP_K = int(os.getenv("RAG_TOP_K", "8"))
    # ...

config = Config()

# Utilisation
model = config.LLM_MODEL
timeout = config.MAX_LLM_TIMEOUT
```

### 3. Logging structur√©

```python
import logging

logger = logging.getLogger(__name__)

# Dans le code
logger.info(f"Parsed context: {len(facts)} facts, {len(corpus)} items")
logger.info(f"RAG: {len(rag_snippets)} snippets retrieved")
logger.info(f"LLM answer length: {len(answer)}")
logger.warning("LLM answer too short, using fallback")
logger.error(f"Ollama connection failed: {e}")
```

## Optimisations

### 1. Embeddings en batch

Au lieu de :
```python
# ‚ùå Lent : 1 appel par document
for doc in docs:
    vec = embed_ollama([doc])
```

Faire :
```python
# ‚úÖ Rapide : 1 seul appel
all_docs = [doc for _, doc in docs]
all_vecs = embed_ollama(all_docs)
```

### 2. Early returns

```python
# Si le corpus est vide, pas besoin de RAG
if not corpus:
    return []

# Si pas assez de temps, skip le RAG
if time_left() < 8.0:
    rag_snippets = []
else:
    rag_snippets = build_rag_context(...)
```

### 3. Cache intelligent

```python
# √âvite de r√©g√©n√©rer la m√™me r√©ponse
RECENT_CACHE = {}  # startup-step ‚Üí [r√©ponses]

# Stockage
def _store_output(key: str, text: str, keep=6):
    RECENT_CACHE.setdefault(key, []).append(text)
    if len(RECENT_CACHE[key]) > keep:
        RECENT_CACHE[key] = RECENT_CACHE[key][-keep:]

# V√©rification
def _too_similar(a: str, b: str) -> bool:
    return jaccard_similarity(a, b) >= 0.90
```

### 4. Timeout adaptatif

```python
# Ajuste le timeout selon le temps restant
max_seconds = min(8.0, time_left() - 1.0)
answer = llm_generate(..., max_seconds=max_seconds)
```

### 5. Mod√®le quantifi√©

Utiliser un mod√®le quantifi√© (q4_K_M) pour la vitesse :
```
llama3.1:8b-instruct ‚Üí 5-8s
llama3.1:8b-instruct-q4_K_M ‚Üí 2-4s (m√™me qualit√© ~95%)
```

## Complexit√© algorithmique

| Op√©ration | Complexit√© | Notes |
|-----------|-----------|-------|
| Parse contexte | O(n) | n = lignes de contexte |
| Embeddings | O(m √ó d) | m = docs, d = dimension |
| Similarit√© cosinus | O(d) | d = dimension vecteur |
| Tri scores | O(m log m) | m = nombre de docs |
| D√©duplication | O(k¬≤) | k = top-k (petit) |
| LLM g√©n√©ration | O(tokens) | D√©pend du mod√®le |
| Scoring | O(items) | Lin√©aire |

**Total pour /generate :**
- Meilleur cas : O(n) (pas de RAG, fallback direct)
- Cas moyen : O(n + m log m + tokens) ‚âà 3-8 secondes
- Pire cas : O(n + m log m + tokens + retry) ‚âà 8-15 secondes

## Diagrammes de flux

### Flux de g√©n√©ration

```
START
  ‚Üì
Parse contexte
  ‚Üì
Temps > 8s? ‚îÄNO‚Üí Skip RAG
  ‚Üì YES
RAG (embeddings + search)
  ‚Üì
Temps > 4.5s? ‚îÄNO‚Üí Skip LLM
  ‚Üì YES
G√©n√©rer avec LLM
  ‚Üì
R√©ponse courte? ‚îÄYES‚Üí Fallback r√®gles
  ‚Üì NO
Nettoyer + Coercer type
  ‚Üì
Similaire au cache? ‚îÄYES‚Üí R√©g√©n√©rer
  ‚Üì NO
Stocker dans cache
  ‚Üì
RETURN r√©ponse
```

### Flux de scoring

```
START
  ‚Üì
Pour chaque item:
  ‚Üì
Analyser contenu (mots, nombres, symboles)
  ‚Üì
Appliquer r√®gles m√©tier
  ‚Üì
Calculer score (0-100)
  ‚Üì
Identifier √©l√©ments manquants
  ‚Üì
G√©n√©rer feedback
  ‚Üì
NEXT item
  ‚Üì
Calculer score global pond√©r√©
  ‚Üì
G√©n√©rer feedback global
  ‚Üì
RETURN r√©sultats
```

## Conclusion

L'algorithme combine :

1. **Intelligence symbolique** (r√®gles m√©tier)
2. **IA g√©n√©rative** (LLM)
3. **Recherche s√©mantique** (RAG)
4. **Optimisations** (cache, batch, timeouts)

Cette approche hybride garantit :
- ‚úÖ Qualit√© (r√©ponses personnalis√©es)
- ‚úÖ Rapidit√© (3-8 secondes)
- ‚úÖ Robustesse (fallbacks multiples)
- ‚úÖ Coh√©rence (anti-duplication)

---

Pour toute question sur l'algorithme, consulter le code source avec les commentaires d√©taill√©s dans `app/main.py`.
