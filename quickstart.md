# âš¡ DÃ©marrage rapide - 5 minutes

Guide ultra-rapide pour dÃ©marrer avec l'API en moins de 5 minutes.

## ğŸ“‹ PrÃ©requis

- Python 3.10+
- 4GB RAM minimum
- 10GB espace disque (pour les modÃ¨les)

## ğŸš€ Installation en 4 Ã©tapes

### 1. Installer Ollama

```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows : tÃ©lÃ©charger depuis https://ollama.com
# Puis redÃ©marrer le terminal
```

### 2. Cloner et configurer

```bash
# Cloner le projet
git clone <votre-repo>
cd startup-incubation-api

# CrÃ©er l'environnement virtuel
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou .venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# CrÃ©er la configuration
cp .env.example .env
```

### 3. TÃ©lÃ©charger les modÃ¨les

```bash
# ModÃ¨le LLM (rapide, ~2GB)
ollama pull llama3.2:3b
# OU si pas disponible : ollama pull mistral:7b-instruct

# ModÃ¨le d'embeddings (~270MB)
ollama pull nomic-embed-text
```

### 4. Lancer !

```bash
# DÃ©marrer l'API
python app/main.py

# âœ… API accessible sur http://localhost:5005
```

## ğŸ§ª Test rapide

Dans un autre terminal :

```bash
# Health check
curl http://localhost:5005/health

# Test de gÃ©nÃ©ration
curl -X POST http://localhost:5005/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "step_name": "Test",
    "question_label": "Quel est votre client cible ?",
    "context": "Startup: TestCo\nSector: FinTech\nPersonas: Freelances",
    "question_type": "text"
  }'
```

**RÃ©sultat attendu :**
```json
{
  "answer": "Les freelances constituent le client cible...",
  "metadata": {...}
}
```

## ğŸ³ Alternative : Docker (recommandÃ© pour production)

```bash
# Lancer tout avec Docker Compose
docker-compose up -d

# TÃ©lÃ©charger les modÃ¨les dans le container
docker exec -it startup-ollama ollama pull llama3.1:3b-instruct
docker exec -it startup-ollama ollama pull nomic-embed-text

# VÃ©rifier
curl http://localhost:5005/health
```

## ğŸ“š Prochaines Ã©tapes

1. **Documentation complÃ¨te** : [README.md](README.md)
2. **Comprendre l'algorithme** : [ALGORITHM.md](ALGORITHM.md)
3. **IntÃ©grer avec Laravel** : [MIGRATION.md](MIGRATION.md#adaptation-de-lintÃ©gration-laravel)
4. **Explorer l'API** : http://localhost:5005/docs

## ğŸ¯ Cas d'usage rapides

### GÃ©nÃ©rer une rÃ©ponse

```python
import requests

response = requests.post('http://localhost:5005/api/generate', json={
    'step_name': 'Personas & Segmentation',
    'question_label': 'Qui est votre client cible ?',
    'context': '''
Startup: MonApp
Sector: EdTech
Problem: Les Ã©tudiants ont du mal Ã  organiser leurs rÃ©visions
Solution: App mobile avec planning intelligent
Personas: Ã‰tudiants universitaires, 18-25 ans
    ''',
    'question_type': 'textarea'
})

print(response.json()['answer'])
```

### Scorer des rÃ©ponses

```python
import requests

response = requests.post('http://localhost:5005/api/score_v2', json={
    'step_name': 'Business Model',
    'items': [
        {
            'question_id': 1,
            'label': 'Qui est impactÃ© ?',
            'type': 'text',
            'answer': 'Les Ã©tudiants universitaires en France, environ 2.7M personnes.',
            'points': 10
        },
        {
            'question_id': 2,
            'label': 'Revenu annÃ©e 1 ?',
            'type': 'number',
            'answer': '50000',
            'points': 5
        }
    ]
})

result = response.json()
print(f"Score global: {result['global_score']}%")
print(f"Status: {result['status']}")
```

## âš™ï¸ Configuration rapide

Ã‰diter `.env` pour personnaliser :

```env
# ModÃ¨le LLM (choisir selon vos besoins)
LLM_MODEL=llama3.1:3b-instruct  # Rapide âœ…
# LLM_MODEL=mistral:7b-instruct  # Meilleure qualitÃ© mais plus lent

# Timeouts (ajuster selon votre machine)
MAX_LLM_TIMEOUT=12.0  # Augmenter si lent
MAX_EMBED_TIMEOUT=6.0

# Port
PORT=5005  # Changer si dÃ©jÃ  utilisÃ©
```

## ğŸ”§ Troubleshooting rapide

### ProblÃ¨me : "Ollama not found"

```bash
# VÃ©rifier qu'Ollama est installÃ©
ollama --version

# VÃ©rifier qu'Ollama tourne
curl http://localhost:11434/api/tags
```

### ProblÃ¨me : "Model not found"

```bash
# Lister les modÃ¨les
ollama list

# TÃ©lÃ©charger le modÃ¨le manquant
ollama pull llama3.1:3b-instruct
```

### ProblÃ¨me : RÃ©ponses lentes (>10s)

```bash
# Option 1 : Utiliser un modÃ¨le plus petit (dÃ©jÃ  le cas)
# Option 2 : VÃ©rifier les ressources
htop  # Linux
# Regarder CPU et RAM

# Option 3 : Augmenter le timeout
# Dans .env : MAX_LLM_TIMEOUT=20.0
```

### ProblÃ¨me : Port dÃ©jÃ  utilisÃ©

```bash
# Trouver ce qui utilise le port 5005
lsof -i :5005  # Linux/Mac
netstat -ano | findstr :5005  # Windows

# Tuer le processus ou changer le port dans .env
PORT=5006
```

## ğŸ‰ C'est tout !

Vous Ãªtes prÃªt Ã  utiliser l'API. Pour aller plus loin :

- **Documentation API interactive** : http://localhost:5005/docs
- **Guide complet** : [README.md](README.md)
- **Support** : CrÃ©er une issue sur GitHub

---

**Temps de setup :** ~5 minutes (hors tÃ©lÃ©chargement des modÃ¨les)  
**Version :** 3.0.0  
**DerniÃ¨re mise Ã  jour :** 2025-10-03