# ğŸš€ Startup Incubation AI Analysis API

API d'analyse et de gÃ©nÃ©ration de rÃ©ponses intelligentes pour accompagner les startups en incubation.

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [Architecture](#architecture)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [API Reference](#api-reference)
- [Algorithme dÃ©taillÃ©](#algorithme-dÃ©taillÃ©)
- [DÃ©ploiement](#dÃ©ploiement)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Vue d'ensemble

Cette API utilise l'intelligence artificielle (via Ollama) pour :
- **GÃ©nÃ©rer des rÃ©ponses personnalisÃ©es** pour les startups en formation
- **Scorer et Ã©valuer** la qualitÃ© des rÃ©ponses fournies
- **Fournir du feedback constructif** avec des suggestions d'amÃ©lioration

### Technologies utilisÃ©es

- **FastAPI** : Framework web moderne et performant
- **Ollama** : LLM local (Llama 3.1, Mistral)
- **Python 3.10+** : Langage principal
- **RAG** (Retrieval-Augmented Generation) : Pour des rÃ©ponses contextuelles

## âœ¨ FonctionnalitÃ©s

### 1. GÃ©nÃ©ration de rÃ©ponses (`/api/generate`)

- Parse le contexte fourni (profil startup, rÃ©ponses prÃ©cÃ©dentes)
- Utilise le RAG pour trouver les informations pertinentes
- GÃ©nÃ¨re une rÃ©ponse personnalisÃ©e via LLM
- Applique des fallbacks basÃ©s sur rÃ¨gles si nÃ©cessaire
- Ã‰vite la duplication avec les rÃ©ponses prÃ©cÃ©dentes

### 2. Scoring structurÃ© (`/api/score_v2`)

- Ã‰value la qualitÃ© de chaque rÃ©ponse (0-100)
- Calcule un score global pondÃ©rÃ©
- Fournit un feedback dÃ©taillÃ©
- Propose des suggestions d'amÃ©lioration
- Identifie les Ã©lÃ©ments manquants

### 3. Health check (`/health`)

- VÃ©rifie l'Ã©tat de l'API
- Teste la connexion Ã  Ollama
- Retourne les modÃ¨les configurÃ©s

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Client (Laravel)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ HTTP/JSON
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Application                        â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Endpoints  â”‚  â”‚   RAG Engine â”‚  â”‚   Scoring    â”‚      â”‚
â”‚  â”‚  /generate   â”‚  â”‚  - Embeddingsâ”‚  â”‚  - Rules     â”‚      â”‚
â”‚  â”‚  /score_v2   â”‚  â”‚  - Search    â”‚  â”‚  - LLM eval  â”‚      â”‚
â”‚  â”‚  /health     â”‚  â”‚  - Dedupe    â”‚  â”‚  - Feedback  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                  â”‚                                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                                                â”‚              â”‚
â”‚                                         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                                         â”‚   Cache      â”‚     â”‚
â”‚                                         â”‚  (In-memory) â”‚     â”‚
â”‚                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ HTTP API
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Ollama Server                           â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  LLM Model       â”‚           â”‚  Embedding Model â”‚        â”‚
â”‚  â”‚  (llama3.1:3b)   â”‚           â”‚  (nomic-embed)   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10+
- Ollama installÃ© et lancÃ©
- Les modÃ¨les Ollama tÃ©lÃ©chargÃ©s

### 1. Installer Ollama

```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows : tÃ©lÃ©charger depuis https://ollama.com
```

### 2. TÃ©lÃ©charger les modÃ¨les

```bash
# ModÃ¨le LLM (choisir un)
ollama pull llama3.1:3b-instruct       # Rapide, recommandÃ©
ollama pull mistral:7b-instruct-q4_K_M # Meilleure qualitÃ©

# ModÃ¨le d'embeddings
ollama pull nomic-embed-text
```

### 3. Cloner et installer l'application

```bash
# Cloner le projet
git clone <votre-repo>
cd startup-incubation-api

# CrÃ©er un environnement virtuel
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate     # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### 4. Configuration

```bash
# Copier le fichier d'exemple
cp .env.example .env

# Ã‰diter .env avec vos paramÃ¨tres
nano .env
```

## âš™ï¸ Configuration

Toute la configuration se fait via le fichier `.env`. Voir `.env.example` pour les dÃ©tails.

### Configuration minimale

```env
OLLAMA_URL=http://localhost:11434
LLM_MODEL=llama3.1:3b-instruct
EMBED_MODEL=nomic-embed-text
PORT=5005
```

### Configuration production

```env
# Serveur
HOST=0.0.0.0
PORT=5005

# Ollama (adapter selon votre dÃ©ploiement)
OLLAMA_URL=http://ollama-service:11434
LLM_MODEL=llama3.1:3b-instruct
EMBED_MODEL=nomic-embed-text

# Timeouts (augmenter si nÃ©cessaire)
MAX_LLM_TIMEOUT=15.0
MAX_EMBED_TIMEOUT=8.0

# CORS (spÃ©cifier vos domaines)
CORS_ORIGINS=https://votre-domaine.com,https://app.votre-domaine.com

# Logging
LOG_LEVEL=INFO
```

## ğŸ® Utilisation

### DÃ©marrer le serveur

```bash
# Mode dÃ©veloppement
uvicorn app.main:app --reload --host 0.0.0.0 --port 5005

# Ou directement
python app/main.py
```

### VÃ©rifier le statut

```bash
curl http://localhost:5005/health
```

### Documentation interactive

Ouvrir dans le navigateur :
- Swagger UI : `http://localhost:5005/docs`
- ReDoc : `http://localhost:5005/redoc`

## ğŸ“¡ API Reference

### POST /api/generate

GÃ©nÃ¨re une rÃ©ponse personnalisÃ©e pour une question.

**Request:**
```json
{
  "step_name": "Personas & Segmentation",
  "question_label": "Qui est impactÃ© par ce problÃ¨me ?",
  "startup_name": "MonStartup",
  "sector_name": "SaaS B2B",
  "context": "Startup: MonStartup\nSector: SaaS B2B\nProblem: Gestion complexe des stocks\n...",
  "question_type": "textarea",
  "prompt": "RÃ©ponse en puces"
}
```

**Response:**
```json
{
  "answer": "- PME du secteur retail (50-200 employÃ©s)\n- Managers de supply chain\n- Ã‰quipes opÃ©rationnelles",
  "metadata": {
    "elapsed_seconds": 2.34,
    "rag_snippets": 5,
    "style": "bullets",
    "question_type": "textarea"
  }
}
```

### POST /api/score_v2

Ã‰value un ensemble de rÃ©ponses structurÃ©es.

**Request:**
```json
{
  "step_name": "Ã‰tape 1",
  "items": [
    {
      "question_id": 1,
      "label": "Question 1",
      "type": "text",
      "answer": "Ma rÃ©ponse...",
      "points": 10
    }
  ]
}
```

**Response:**
```json
{
  "items": [
    {
      "question_id": 1,
      "score": 75,
      "status": "validÃ©e",
      "feedback": "Bon niveau. Rendez 1â€“2 Ã©lÃ©ments plus concrets.",
      "points": 10,
      "issues": [],
      "suggestion_bullets": ["â€¢ Ajouter des chiffres"],
      "suggested_answer": "Ex : ..."
    }
  ],
  "global_score": 75,
  "status": "validÃ©e",
  "feedback": "Niveau satisfaisant (75%)."
}
```

### GET /health

VÃ©rifie l'Ã©tat de l'API.

**Response:**
```json
{
  "status": "ok",
  "version": "3.0.0",
  "ollama_status": "ok",
  "model": "llama3.1:3b-instruct",
  "embed_model": "nomic-embed-text"
}
```

## ğŸ§  Algorithme dÃ©taillÃ©

Voir le fichier [ALGORITHM.md](ALGORITHM.md) pour une explication dÃ©taillÃ©e de l'algorithme.

### Pipeline de gÃ©nÃ©ration (rÃ©sumÃ©)

1. **Parse du contexte** : Extraction des faits et du corpus
2. **RAG** : Recherche sÃ©mantique des snippets pertinents
3. **GÃ©nÃ©ration LLM** : CrÃ©ation de la rÃ©ponse avec prompt enrichi
4. **Fallbacks** : Application de rÃ¨gles si rÃ©ponse insuffisante
5. **Post-traitement** : Nettoyage, coercition, anti-duplication

### Pipeline de scoring (rÃ©sumÃ©)

1. **Analyse du contenu** : Mots, chiffres, symboles
2. **Calcul du score** : BasÃ© sur des rÃ¨gles mÃ©tier
3. **Feedback** : GÃ©nÃ©ration de suggestions
4. **Score global** : Moyenne pondÃ©rÃ©e

## ğŸ³ DÃ©ploiement

### Docker Compose (recommandÃ©)

CrÃ©er `docker-compose.yml` :

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    
  api:
    build: .
    container_name: startup-api
    ports:
      - "5005:5005"
    environment:
      - OLLAMA_URL=http://ollama:11434
      - LLM_MODEL=llama3.1:3b-instruct
      - EMBED_MODEL=nomic-embed-text
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:
```

Lancer :
```bash
docker-compose up -d

# TÃ©lÃ©charger les modÃ¨les dans le container Ollama
docker exec -it ollama ollama pull llama3.1:3b-instruct
docker exec -it ollama ollama pull nomic-embed-text
```

### Dockerfile amÃ©liorÃ©

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Installer les dÃ©pendances systÃ¨me
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copier les fichiers
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app ./app
COPY .env .env

# Exposer le port
EXPOSE 5005

# Healthcheck
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:5005/health || exit 1

# Lancer l'application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "5005"]
```

## ğŸ”§ Troubleshooting

### ProblÃ¨me : Ollama ne rÃ©pond pas

**SymptÃ´mes :**
```json
{"status": "ok", "ollama_status": "error"}
```

**Solutions :**
1. VÃ©rifier qu'Ollama est lancÃ© : `ollama list`
2. VÃ©rifier l'URL dans `.env`
3. Tester manuellement : `curl http://localhost:11434/api/tags`

### ProblÃ¨me : Timeout lors de la gÃ©nÃ©ration

**SymptÃ´mes :**
- RÃ©ponses vides
- Erreurs de timeout dans les logs

**Solutions :**
1. Augmenter `MAX_LLM_TIMEOUT` dans `.env`
2. Utiliser un modÃ¨le plus petit (3b au lieu de 7b)
3. VÃ©rifier les ressources systÃ¨me (CPU/RAM)

### ProblÃ¨me : RÃ©ponses de mauvaise qualitÃ©

**Solutions :**
1. Utiliser un modÃ¨le plus gros : `mistral:7b-instruct`
2. Augmenter `RAG_TOP_K` pour plus de contexte
3. AmÃ©liorer le contexte fourni dans les requÃªtes

### ProblÃ¨me : CORS errors

**SymptÃ´mes :**
```
Access to XMLHttpRequest at 'http://localhost:5005/api/generate' 
from origin 'http://localhost:8000' has been blocked by CORS policy
```

**Solutions :**
1. Ajouter l'origine dans `CORS_ORIGINS` dans `.env`
2. Exemple : `CORS_ORIGINS=http://localhost:8000,https://app.example.com`

## ğŸ“Š Performance

### Benchmarks (modÃ¨le 3b, CPU moderne)

| OpÃ©ration | Temps moyen | Notes |
|-----------|-------------|-------|
| RAG (8 snippets) | 1-2s | DÃ©pend du corpus |
| GÃ©nÃ©ration LLM | 2-5s | DÃ©pend de la longueur |
| Scoring | <0.5s | BasÃ© sur rÃ¨gles |
| Total /generate | 3-8s | Avec RAG |

### Optimisations possibles

1. **GPU** : Utiliser Ollama avec GPU (x5-10 plus rapide)
2. **Cache** : DÃ©jÃ  implÃ©mentÃ© (Ã©vite les duplications)
3. **ModÃ¨le quantifiÃ©** : q4_K_M pour rapiditÃ©
4. **Batch processing** : Pour scorer plusieurs items

## ğŸ¤ Contribution

Les contributions sont bienvenues !

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/AmazingFeature`)
3. Commit (`git commit -m 'Add AmazingFeature'`)
4. Push (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“ License

Ce projet est sous licence MIT.

## ğŸ“§ Contact

Pour toute question : [votre-email@example.com]

---

**Version :** 3.0.0  
**DerniÃ¨re mise Ã  jour :** 2025
