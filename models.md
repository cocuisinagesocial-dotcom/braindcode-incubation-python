# ü§ñ Guide des mod√®les Ollama

Ce guide vous aide √† choisir les bons mod√®les pour votre cas d'usage.

## üìä Comparaison rapide

| Mod√®le | Taille | Vitesse | Qualit√© | RAM | Recommand√© pour |
|--------|--------|---------|---------|-----|-----------------|
| **llama3.2:3b** | 2GB | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 4GB | Production, API rapide ‚úÖ |
| **gemma2:2b** | 1.6GB | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 3GB | Ultra-rapide, contraintes |
| **mistral:7b** | 4GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | Qualit√© optimale |
| **llama3.1:8b** | 4.7GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | Meilleur √©quilibre |
| **qwen2.5:7b** | 4.7GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | Excellent alternatif |
| **phi3:3.8b** | 2.3GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 4GB | Bon compromis |

## üéØ Choix selon votre cas

### Vous avez peu de RAM (4GB)
```bash
# Option 1 : Le plus rapide et l√©ger
ollama pull llama3.2:3b

# Option 2 : Ultra-l√©ger
ollama pull gemma2:2b

# Configuration .env
LLM_MODEL=llama3.2:3b
```

### Vous voulez la meilleure qualit√© (8GB+)
```bash
# Option 1 : Excellent fran√ßais
ollama pull mistral:7b-instruct

# Option 2 : Tr√®s polyvalent
ollama pull llama3.1:8b

# Option 3 : Alternative excellente
ollama pull qwen2.5:7b

# Configuration .env
LLM_MODEL=mistral:7b-instruct
```

### Vous voulez un bon compromis (6GB)
```bash
# Option 1 : Microsoft Phi
ollama pull phi3:3.8b

# Option 2 : Llama 3.2
ollama pull llama3.2:3b

# Configuration .env
LLM_MODEL=phi3:3.8b
```

### Vous avez un GPU
```bash
# Vous pouvez utiliser des mod√®les plus gros
ollama pull llama3.1:8b
ollama pull mistral:7b-instruct

# Ou m√™me des quantifications non compress√©es
ollama pull mistral:7b-instruct-q8_0

# Configuration .env
LLM_MODEL=mistral:7b-instruct
```

## üîç V√©rifier les mod√®les disponibles

### Voir ce qui est install√©
```bash
ollama list
```

### Chercher des mod√®les
```bash
# Chercher des mod√®les Llama
ollama search llama

# Chercher des mod√®les Mistral
ollama search mistral

# Voir tous les mod√®les populaires
# Visitez : https://ollama.com/library
```

## üì• Installation des mod√®les

### Llama 3.2 3B (recommand√©)
```bash
ollama pull llama3.2:3b
```

### Mistral 7B
```bash
# Version de base
ollama pull mistral:7b

# Version instruct (meilleure pour notre usage)
ollama pull mistral:7b-instruct

# Version quantifi√©e Q4 (plus rapide)
ollama pull mistral:7b-instruct-q4_K_M
```

### Gemma 2 2B (ultra-rapide)
```bash
ollama pull gemma2:2b
```

### Llama 3.1 8B
```bash
ollama pull llama3.1:8b
```

### Qwen 2.5 7B
```bash
ollama pull qwen2.5:7b
```

### Phi 3 3.8B
```bash
ollama pull phi3:3.8b
```

## üß™ Tester un mod√®le

```bash
# Test interactif
ollama run llama3.2:3b

# Dans le prompt :
>>> Tu es un coach pour startups. Explique ce qu'est une proposition de valeur.

# Quitter : /bye
```

## ‚öôÔ∏è Configuration dans l'API

### Fichier .env
```env
# Mod√®le principal
LLM_MODEL=llama3.2:3b

# Mod√®le d'embeddings (ne pas changer)
EMBED_MODEL=nomic-embed-text

# Augmenter le timeout si mod√®le lent
MAX_LLM_TIMEOUT=15.0
```

### Tester avec l'API
```bash
# D√©marrer l'API
python app/main.py

# Dans un autre terminal
curl http://localhost:5005/health
```

Vous devriez voir :
```json
{
  "status": "ok",
  "ollama_status": "ok",
  "model": "llama3.2:3b",
  "embed_model": "nomic-embed-text"
}
```

## üîÑ Changer de mod√®le

### M√©thode 1 : Via .env
```bash
# √âditer .env
nano .env

# Changer LLM_MODEL
LLM_MODEL=mistral:7b-instruct

# Red√©marrer l'API
```

### M√©thode 2 : Variable d'environnement
```bash
# Linux/Mac
export LLM_MODEL=mistral:7b-instruct
python app/main.py

# Windows
set LLM_MODEL=mistral:7b-instruct
python app/main.py
```

### M√©thode 3 : Docker
```yaml
# docker-compose.yml
services:
  api:
    environment:
      - LLM_MODEL=mistral:7b-instruct
```

## üìä Benchmarks sur notre API

Tests sur un CPU moderne (Intel i7) avec diff√©rents mod√®les :

| Mod√®le | G√©n√©ration simple | G√©n√©ration + RAG | M√©moire |
|--------|-------------------|------------------|---------|
| llama3.2:3b | 2-3s | 4-6s | 3.5GB |
| gemma2:2b | 1-2s | 3-5s | 2.8GB |
| mistral:7b | 4-6s | 7-10s | 6GB |
| llama3.1:8b | 5-7s | 8-12s | 7GB |
| phi3:3.8b | 2-4s | 5-8s | 4GB |

**Note :** Avec GPU, divisez ces temps par 5-10x.

## üêõ Probl√®mes courants

### "Error: pull model manifest: file does not exist"

**Cause :** Le mod√®le n'existe pas avec ce nom.

**Solution :**
```bash
# V√©rifier le nom exact sur https://ollama.com/library
# Exemples de noms corrects :
ollama pull llama3.2:3b          # ‚úÖ
ollama pull llama3.2              # ‚úÖ (version par d√©faut)
ollama pull llama3.1:3b-instruct # ‚ùå N'EXISTE PAS
```

### "Error: out of memory"

**Solution 1 :** Utiliser un mod√®le plus petit
```bash
ollama pull gemma2:2b
```

**Solution 2 :** Augmenter la RAM allou√©e √† Docker
```bash
# Dans Docker Desktop : Settings > Resources > Memory
# Allouer au moins 8GB
```

**Solution 3 :** Utiliser une version quantifi√©e
```bash
# q4_K_M = quantification 4 bits
ollama pull mistral:7b-instruct-q4_K_M
```

### Le mod√®le est tr√®s lent

**Solution 1 :** Utiliser un mod√®le plus petit
```bash
ollama pull llama3.2:3b
```

**Solution 2 :** Augmenter le timeout
```env
# Dans .env
MAX_LLM_TIMEOUT=20.0
```

**Solution 3 :** V√©rifier que vous n'√™tes pas en mode swap
```bash
# Linux
free -h

# Si swap utilis√©, ajouter de la RAM ou r√©duire le mod√®le
```

### La qualit√© n'est pas bonne

**Solution :** Utiliser un mod√®le plus gros
```bash
# Passer de 3B √† 7B
ollama pull mistral:7b-instruct

# Ou √† 8B
ollama pull llama3.1:8b
```

## üéì Recommandations finales

### Pour commencer (QUICKSTART)
```bash
ollama pull llama3.2:3b
ollama pull nomic-embed-text
```

### Pour production
```bash
# Si serveur avec 8GB+ RAM
ollama pull mistral:7b-instruct
ollama pull nomic-embed-text

# Si serveur avec 4-6GB RAM
ollama pull llama3.2:3b
ollama pull nomic-embed-text
```

### Pour d√©veloppement
```bash
# Version rapide pour it√©rer vite
ollama pull gemma2:2b
ollama pull nomic-embed-text
```

### Pour qualit√© maximale
```bash
# Si GPU disponible
ollama pull llama3.1:8b
ollama pull mistral:7b-instruct
ollama pull nomic-embed-text
```

## üìö Ressources

- **Biblioth√®que Ollama :** https://ollama.com/library
- **Documentation Ollama :** https://github.com/ollama/ollama
- **Comparaisons :** https://ollama.com/blog

---

**Derni√®re mise √† jour :** 2025-10-03  
**Version API :** 3.0.0
